# Example config for LLM training
model_name: HuggingFaceTB/SmolLM2-135M  # Model name or path
run_name: "tinystories_ncp_2"  # Unique run name for this experiment
run_dir: "/home/aiscuser/experiments/outputs/"
seed: 42 

data:
  processed_path: "/home/aiscuser/experiments/outputs/tokenized_datasets/tinystories_ncp_1/"
  dataset_name: roneneldan/TinyStories
  split: train
  streaming: false
  max_length: 512
  append_eos: true
  context_packing: false

training:
  pretrain_from_scratch: true
  init_method: 'kaiming_normal' #['xavier_uniform', 'kaiming_normal', 'normal']
  lr: 1e-3
  weight_decay: 0.01
  eps: 1e-8
  beta1: 0.9
  beta2: 0.98
  batch_size: 100
  gradient_accumulation_steps: 4
  epochs: 10
  bf16: true
  fp16: false 
  ddp_find_unused_parameters: false
  max_grad_norm: 1.0
  dataloader_drop_last: true
  
scheduler:
  name: 'cosine'
  warmup_ratio: 0.0
  num_cycles: 0.5
  decay_ratio: 0.1
  warmup_type: 'linear'
  decay_type: 'linear'
  min_lr_ratio: 0.1

logging:
  overwrite_output_dir: true
  save_steps_ratio: 0.25
  save_total_limit: 5
  logging_steps: 1
  report_to: 'wandb'
  save_strategy: 'epoch'
  logging_strategy: 'steps'

