# Example config for LLM training
model_name: roneneldan/TinyStories-33M  # Model name or path
run_name: "tinystories_33M_rep6"  # Unique run name for this experiment
run_dir: "/datadrive/pavan/experiments/outputs/"
seed: 42 

data:
  processed_path: "/datadrive/pavan/experiments/outputs/tokenized_datasets/tinystories_33M_rep"
  dataset_name: roneneldan/TinyStories
  split: train
  streaming: false
  max_length: 512
  append_eos: true
  context_packing: false
  column_name: 'text'

training:
  pretrain_from_scratch: true
  init_method: 'kaiming_normal' #['xavier_uniform', 'kaiming_normal', 'normal']
  lr: 5e-4
  weight_decay: 0.1
  eps: 5e-4
  beta1: 0.9
  beta2: 0.95
  batch_size: 80
  gradient_accumulation_steps: 4
  epochs: 10
  bf16: true
  fp16: false 
  ddp_find_unused_parameters: false
  max_grad_norm: 1.0
  dataloader_drop_last: true
  
scheduler:
  name: 'constant_with_warmup'
  warmup_ratio: 0.0
  num_cycles: 0.5
  decay_ratio: 0.1
  warmup_type: 'linear'
  decay_type: 'linear'
  min_lr_ratio: 0.1

logging:
  overwrite_output_dir: true
  save_steps_ratio: 0.25
  save_total_limit: 5
  logging_steps: 1
  report_to: 'wandb'
  save_strategy: 'epoch'
  logging_strategy: 'steps'

