# Example config for LLM training
model_size: 150M  # Options: 150M, 360M
model_name: null  # Optional: override with a specific HuggingFace model name
run_name: "tinystories_cp_150"  # Unique run name for this experiment
run_dir: "./outputs/"

data:
  dataset_name: roneneldan/TinyStories #wikitext
  # dataset_config_name: wikitext-2-raw-v1  # Added config name for wikitext
  split: train
  streaming: false
  # chat_template: ./example_chat_template.txt  # Use the example chat template
  tokenize: true

training:
  learning_rate: 1e-4
  weight_decay: 0.01
  lr_scheduler_type: "cosine"  # or "cosine"
  optimizer: "adafactor"
  type: pretrain_chat  # Use chat template pretraining
  epochs: 4
  batch_size: 32
  save_steps: 1000
  max_length: 2048
  gradient_accumulation_steps: 32
  fp16: false
  context_packing: false  # Whether to enable context packing
  pretrain_from_scratch: true 
wandb:
  enabled: true
  project: "llm-training"
