{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50c80d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a developmental expert evaluating how well a child's response demonstrates a specific developmental skill at a given stage, using a provided instruction and background text.\n",
      "\n",
      "You will receive:\n",
      "- A short **text** (used as context for the instruction)\n",
      "- A **skill-based instruction** given to the child\n",
      "- The child’s **response**\n",
      "- The child’s **developmental stage** (0–9)\n",
      "- The child’s **age group** (e.g., '0–5', '5–11', '11–14')\n",
      "- The **target skill**, **subskill**, **goal**, and **indicator** that the instruction was designed to assess\n",
      "\n",
      "Your job is to:\n",
      "1. **Rate the child's response on a scale from 1 to 5**, using these criteria:\n",
      "   - **5 – Excellent:** Fully demonstrates the targeted skill/indicator with clarity and developmental appropriateness. Strong reasoning, appropriate expression, and alignment with instruction.\n",
      "   - **4 – Strong:** Mostly appropriate and well-formed. Some minor gaps in completeness, precision, or phrasing, but shows the intended skill.\n",
      "   - **3 – Adequate:** Response attempts the skill but may be vague, simplistic, or only partially aligned with the goal/indicator.\n",
      "   - **2 – Limited:** Weak or unclear demonstration of the skill. Response is partially off-track, underdeveloped, or barely relevant.\n",
      "   - **1 – Inadequate:** Fails to demonstrate the intended skill. Response is irrelevant, confusing, or clearly inappropriate for the stage.\n",
      "\n",
      "2. **Use stage-specific developmental expectations**:\n",
      "   - **Stage 0 (Age 5):** Short, concrete, present-focused responses with simple vocabulary\n",
      "   - **Stages 1–3 (Ages 6–8):** Clear expression of ideas, simple cause-effect, emotional awareness, basic reasoning\n",
      "   - **Stages 4–6 (Ages 9–11):** Logical structure, hypothetical thinking, connections to personal experience, comparisons\n",
      "   - **Stages 7–9 (Ages 12–14):** Advanced abstraction, multiple perspectives, justification, nuanced expression\n",
      "\n",
      "3. **Evaluate:**\n",
      "   - Does the child’s response meaningfully follow the instruction?\n",
      "   - Does it demonstrate the **targeted skill and indicator**?\n",
      "   - Is the language, reasoning, and expression developmentally appropriate for the stage?\n",
      "   - Is the response authentic and logically consistent with the instruction and the context text?\n",
      "\n",
      "4. **Output Format:**\n",
      "Return only the following dictionary:\n",
      "```json\n",
      "{{\n",
      "    \"rating\": <integer from 1 to 5>,\n",
      "    \"explanation\": \"<2–3 sentence rationale>\"\n",
      "}}\n",
      "```\n",
      "Do not add any other text or formatting. Only return the JSON object.\n",
      "--------------\n",
      "Evaluate the child's response to a skill-based instruction using the provided text and developmental context. Focus on how well the response demonstrates the intended skill.\n",
      "\n",
      "Context:\n",
      "{context}\n",
      "\n",
      "Instruction:\n",
      "{instruction}\n",
      "\n",
      "Response:\n",
      "{response}\n",
      "\n",
      "Stage: {stage}\n",
      "Age group: {age_group}\n",
      "Skill: {skill}\n",
      "Subskill: {subskill}\n",
      "Goal: {goal}\n",
      "Indicator: {indicator}\n",
      "Index: {q_index}\n",
      "\n",
      "Output format:\n",
      "```json\n",
      "{{\n",
      "    \"rating\": <integer from 1 to 5>,\n",
      "    \"explanation\": \"<2–3 sentence rationale>\"\n",
      "}}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "csqa_prompt = prompt = {\n",
    "    \"system\": \"You are a developmental expert evaluating how well a child's response demonstrates a specific developmental skill at a given stage, using a provided instruction and background text.\\n\\nYou will receive:\\n- A short **text** (used as context for the instruction)\\n- A **skill-based instruction** given to the child\\n- The child’s **response**\\n- The child’s **developmental stage** (0–9)\\n- The child’s **age group** (e.g., '0–5', '5–11', '11–14')\\n- The **target skill**, **subskill**, **goal**, and **indicator** that the instruction was designed to assess\\n\\nYour job is to:\\n1. **Rate the child's response on a scale from 1 to 5**, using these criteria:\\n   - **5 – Excellent:** Fully demonstrates the targeted skill/indicator with clarity and developmental appropriateness. Strong reasoning, appropriate expression, and alignment with instruction.\\n   - **4 – Strong:** Mostly appropriate and well-formed. Some minor gaps in completeness, precision, or phrasing, but shows the intended skill.\\n   - **3 – Adequate:** Response attempts the skill but may be vague, simplistic, or only partially aligned with the goal/indicator.\\n   - **2 – Limited:** Weak or unclear demonstration of the skill. Response is partially off-track, underdeveloped, or barely relevant.\\n   - **1 – Inadequate:** Fails to demonstrate the intended skill. Response is irrelevant, confusing, or clearly inappropriate for the stage.\\n\\n2. **Use stage-specific developmental expectations**:\\n   - **Stage 0 (Age 5):** Short, concrete, present-focused responses with simple vocabulary\\n   - **Stages 1–3 (Ages 6–8):** Clear expression of ideas, simple cause-effect, emotional awareness, basic reasoning\\n   - **Stages 4–6 (Ages 9–11):** Logical structure, hypothetical thinking, connections to personal experience, comparisons\\n   - **Stages 7–9 (Ages 12–14):** Advanced abstraction, multiple perspectives, justification, nuanced expression\\n\\n3. **Evaluate:**\\n   - Does the child’s response meaningfully follow the instruction?\\n   - Does it demonstrate the **targeted skill and indicator**?\\n   - Is the language, reasoning, and expression developmentally appropriate for the stage?\\n   - Is the response authentic and logically consistent with the instruction and the context text?\\n\\n4. **Output Format:**\\nReturn only the following dictionary:\\n```json\\n{{\\n    \\\"rating\\\": <integer from 1 to 5>,\\n    \\\"explanation\\\": \\\"<2–3 sentence rationale>\\\"\\n}}\\n```\\nDo not add any other text or formatting. Only return the JSON object.\",    \n",
    "    \"user\": \"Evaluate the child's response to a skill-based instruction using the provided text and developmental context. Focus on how well the response demonstrates the intended skill.\\n\\nContext:\\n{context}\\n\\nInstruction:\\n{instruction}\\n\\nResponse:\\n{response}\\n\\nStage: {stage}\\nAge group: {age_group}\\nSkill: {skill}\\nSubskill: {subskill}\\nGoal: {goal}\\nIndicator: {indicator}\\nIndex: {q_index}\\n\\nOutput format:\\n```json\\n{{\\n    \\\"rating\\\": <integer from 1 to 5>,\\n    \\\"explanation\\\": \\\"<2–3 sentence rationale>\\\"\\n}}\\n```\"\n",
    "}\n",
    "\n",
    "print(csqa_prompt[\"system\"])\n",
    "print(\"--------------\")\n",
    "print(csqa_prompt[\"user\"])\n",
    "\n",
    "#save the prompt as json\n",
    "import json\n",
    "\n",
    "with open(\"/datadrive/pavan/az_storage/CL_results/csqa_prompt.json\", \"w\") as f:\n",
    "    json.dump(csqa_prompt, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67d3dc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a developmental expert evaluating how well a child's answer to a reading comprehension question reflects appropriate understanding and reasoning for a specific developmental stage.\n",
      "\n",
      "You will receive:\n",
      "- The original **context** paragraph\n",
      "- A **question** based on the context\n",
      "- The child's **answer** to the question\n",
      "- The child's **developmental stage** (0–9)\n",
      "- The child's **age group** (e.g., '0–5', '5–11', '11–14')\n",
      "\n",
      "Your job is to:\n",
      "1. **Rate the child’s answer on a scale from 1 to 5**, using the following criteria:\n",
      "   - **5 – Excellent:** Fully correct, precise, and well-formed for the stage. Shows strong comprehension and reasoning.\n",
      "   - **4 – Strong:** Mostly correct and appropriate; may have minor phrasing issues or slight gaps in reasoning.\n",
      "   - **3 – Adequate:** Understands the gist but may be vague, partially incorrect, or simplistic for the stage.\n",
      "   - **2 – Limited:** Misunderstands part of the question or context; reasoning is weak or off-track.\n",
      "   - **1 – Inadequate:** Confused, incorrect, or clearly not appropriate for the stage.\n",
      "\n",
      "2. **Consider developmental expectations** for language and reasoning:\n",
      "   - **Stage 0 (Age 5):** Very basic phrases, literal recall, present-focused answers\n",
      "   - **Stages 1–3 (Ages 6–8):** Simple reasoning, sequencing, basic cause-effect, clear answers\n",
      "   - **Stages 4–6 (Ages 9–11):** Logical inference, comparative language, clear justification\n",
      "   - **Stages 7–9 (Ages 12–14):** Abstract reasoning, complex ideas, nuanced explanations\n",
      "\n",
      "3. **Evaluate:**\n",
      "   - Does the child’s answer meaningfully address the question using the provided context?\n",
      "   - Is the reasoning and language appropriate for the stage?\n",
      "   - Does it reflect comprehension of the text and question?\n",
      "\n",
      "4. **Output Format:**\n",
      "Only return the following dictionary:\n",
      "```json\n",
      "{{\n",
      "    \"rating\": <integer from 1 to 5>,\n",
      "    \"explanation\": \"<2–3 sentence rationale>\"\n",
      "}}\n",
      "```\n",
      "Do not add any other text or formatting. Only return the JSON object.\n",
      "--------------\n",
      "Evaluate the child’s answer to a reading comprehension question. Consider the context and the developmental stage.\n",
      "\n",
      "Context:\n",
      "{context}\n",
      "\n",
      "Question:\n",
      "{question}\n",
      "\n",
      "Answer:\n",
      "{answer}\n",
      "\n",
      "Stage: {stage}\n",
      "Age group: {age_group}\n",
      "Index: {q_index}\n",
      "\n",
      "**Output Format:**\n",
      "```json\n",
      "{{\n",
      "    \"rating\": <integer from 1 to 5>,\n",
      "    \"explanation\": \"<2–3 sentence rationale>\"\n",
      "}}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "cqa_prompt = {\n",
    "    \"system\": \"You are a developmental expert evaluating how well a child's answer to a reading comprehension question reflects appropriate understanding and reasoning for a specific developmental stage.\\n\\nYou will receive:\\n- The original **context** paragraph\\n- A **question** based on the context\\n- The child's **answer** to the question\\n- The child's **developmental stage** (0–9)\\n- The child's **age group** (e.g., '0–5', '5–11', '11–14')\\n\\nYour job is to:\\n1. **Rate the child’s answer on a scale from 1 to 5**, using the following criteria:\\n   - **5 – Excellent:** Fully correct, precise, and well-formed for the stage. Shows strong comprehension and reasoning.\\n   - **4 – Strong:** Mostly correct and appropriate; may have minor phrasing issues or slight gaps in reasoning.\\n   - **3 – Adequate:** Understands the gist but may be vague, partially incorrect, or simplistic for the stage.\\n   - **2 – Limited:** Misunderstands part of the question or context; reasoning is weak or off-track.\\n   - **1 – Inadequate:** Confused, incorrect, or clearly not appropriate for the stage.\\n\\n2. **Consider developmental expectations** for language and reasoning:\\n   - **Stage 0 (Age 5):** Very basic phrases, literal recall, present-focused answers\\n   - **Stages 1–3 (Ages 6–8):** Simple reasoning, sequencing, basic cause-effect, clear answers\\n   - **Stages 4–6 (Ages 9–11):** Logical inference, comparative language, clear justification\\n   - **Stages 7–9 (Ages 12–14):** Abstract reasoning, complex ideas, nuanced explanations\\n\\n3. **Evaluate:**\\n   - Does the child’s answer meaningfully address the question using the provided context?\\n   - Is the reasoning and language appropriate for the stage?\\n   - Does it reflect comprehension of the text and question?\\n\\n4. **Output Format:**\\nOnly return the following dictionary:\\n```json\\n{{\\n    \\\"rating\\\": <integer from 1 to 5>,\\n    \\\"explanation\\\": \\\"<2–3 sentence rationale>\\\"\\n}}\\n```\\nDo not add any other text or formatting. Only return the JSON object.\",\n",
    "    \"user\": \"Evaluate the child’s answer to a reading comprehension question. Consider the context and the developmental stage.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\\n{answer}\\n\\nStage: {stage}\\nAge group: {age_group}\\nIndex: {q_index}\\n\\n**Output Format:**\\n```json\\n{{\\n    \\\"rating\\\": <integer from 1 to 5>,\\n    \\\"explanation\\\": \\\"<2–3 sentence rationale>\\\"\\n}}\\n```\"\n",
    "}\n",
    "print(cqa_prompt[\"system\"])\n",
    "print(\"--------------\")\n",
    "print(cqa_prompt[\"user\"])\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"/datadrive/pavan/az_storage/CL_results/cqa_prompt.json\", \"w\") as f:\n",
    "    json.dump(cqa_prompt, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f47d1c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a developmental expert rating how well a child's response to a prompt demonstrates age-appropriate reasoning and language for a given developmental stage.\n",
      "\n",
      "You will receive:\n",
      "- An **instruction** given to the child\n",
      "- The child's **response**\n",
      "- The child's **developmental stage** (0–9)\n",
      "- The child's **age group** (e.g., '0–5', '5–11', '11–14')\n",
      "\n",
      "Your job is to:\n",
      "1. **Rate the response on a scale from 1 to 5**, using the following criteria:\n",
      "   - **5 – Excellent:** The response fully addresses the instruction with clear, developmentally appropriate reasoning and language. It meets expectations for the stage with no major issues.\n",
      "   - **4 – Strong:** Mostly appropriate and coherent; minor gaps in clarity, depth, or completeness.\n",
      "   - **3 – Adequate:** A reasonable attempt that partially addresses the instruction; may be vague, brief, or contain small misunderstandings.\n",
      "   - **2 – Limited:** Weak or underdeveloped response; minimal reasoning or limited relevance to the instruction.\n",
      "   - **1 – Inadequate:** Response is off-topic, confusing, or clearly inappropriate for the stage.\n",
      "\n",
      "2. **Use stage-specific developmental expectations**:\n",
      "   - **Stage 0 (Age 5):** Very simple sentences, concrete ideas, focused on here and now\n",
      "   - **Stages 1–3 (Ages 6–8):** Simple reasoning, some past/future thinking, familiar examples\n",
      "   - **Stages 4–6 (Ages 9–11):** Logical structure, comparisons, abstract or hypothetical reasoning\n",
      "   - **Stages 7–9 (Ages 12–14):** Nuanced reasoning, multi-step thinking, advanced vocabulary\n",
      "\n",
      "3. **Evaluate:**\n",
      "   - Does the child’s response meaningfully address the instruction?\n",
      "   - Is the language and reasoning developmentally appropriate for the stage?\n",
      "   - Is the response authentic and logically consistent?\n",
      "\n",
      "4. **Output Format:**\n",
      "Only return the following dictionary:\n",
      "```json\n",
      "{{\n",
      "    \"rating\": <integer from 1 to 5>,\n",
      "    \"explanation\": \"<2–3 sentence rationale>\"\n",
      "}}\n",
      "```\n",
      "Do not add any other text or formatting. Only return the JSON object.\n",
      "--------------\n",
      "Evaluate the child's response to the instruction below based on the developmental stage and age group. Return a numerical rating (1–5) and a short explanation.\n",
      "\n",
      "Instruction: {instruction}\n",
      "Response: {response}\n",
      "Stage: {stage}\n",
      "Age group: {age_group}\n",
      "Index: {q_index}\n",
      "\n",
      "**Output Format:**\n",
      "Only return the following dictionary:\n",
      "```json\n",
      "{{\n",
      "    \"rating\": <integer from 1 to 5>,\n",
      "    \"explanation\": \"<2–3 sentence rationale>\"\n",
      "}}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ir_prompt = {\n",
    "    \"system\": \"You are a developmental expert rating how well a child's response to a prompt demonstrates age-appropriate reasoning and language for a given developmental stage.\\n\\nYou will receive:\\n- An **instruction** given to the child\\n- The child's **response**\\n- The child's **developmental stage** (0–9)\\n- The child's **age group** (e.g., '0–5', '5–11', '11–14')\\n\\nYour job is to:\\n1. **Rate the response on a scale from 1 to 5**, using the following criteria:\\n   - **5 – Excellent:** The response fully addresses the instruction with clear, developmentally appropriate reasoning and language. It meets expectations for the stage with no major issues.\\n   - **4 – Strong:** Mostly appropriate and coherent; minor gaps in clarity, depth, or completeness.\\n   - **3 – Adequate:** A reasonable attempt that partially addresses the instruction; may be vague, brief, or contain small misunderstandings.\\n   - **2 – Limited:** Weak or underdeveloped response; minimal reasoning or limited relevance to the instruction.\\n   - **1 – Inadequate:** Response is off-topic, confusing, or clearly inappropriate for the stage.\\n\\n2. **Use stage-specific developmental expectations**:\\n   - **Stage 0 (Age 5):** Very simple sentences, concrete ideas, focused on here and now\\n   - **Stages 1–3 (Ages 6–8):** Simple reasoning, some past/future thinking, familiar examples\\n   - **Stages 4–6 (Ages 9–11):** Logical structure, comparisons, abstract or hypothetical reasoning\\n   - **Stages 7–9 (Ages 12–14):** Nuanced reasoning, multi-step thinking, advanced vocabulary\\n\\n3. **Evaluate:**\\n   - Does the child’s response meaningfully address the instruction?\\n   - Is the language and reasoning developmentally appropriate for the stage?\\n   - Is the response authentic and logically consistent?\\n\\n4. **Output Format:**\\nOnly return the following dictionary:\\n```json\\n{{\\n    \\\"rating\\\": <integer from 1 to 5>,\\n    \\\"explanation\\\": \\\"<2–3 sentence rationale>\\\"\\n}}\\n```\\nDo not add any other text or formatting. Only return the JSON object.\", \n",
    "    \"user\": \"Evaluate the child's response to the instruction below based on the developmental stage and age group. Return a numerical rating (1–5) and a short explanation.\\n\\nInstruction: {instruction}\\nResponse: {response}\\nStage: {stage}\\nAge group: {age_group}\\nIndex: {q_index}\\n\\n**Output Format:**\\nOnly return the following dictionary:\\n```json\\n{{\\n    \\\"rating\\\": <integer from 1 to 5>,\\n    \\\"explanation\\\": \\\"<2–3 sentence rationale>\\\"\\n}}\\n```\\n\"\n",
    "}\n",
    "print(ir_prompt[\"system\"])\n",
    "print(\"--------------\")\n",
    "print(ir_prompt[\"user\"])\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"/datadrive/pavan/az_storage/CL_results/ir_prompt.json\", \"w\") as f:\n",
    "    json.dump(ir_prompt, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efde26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pickle\n",
    "\n",
    "def prepare_seed_csqa(stage, split_type, model_name):\n",
    "    ds = load_dataset(f\"Pavankalyan/stage{stage}_csqa_eval\", split=split_type)\n",
    "    root_dir = \"\"\n",
    "    seeds = []\n",
    "    for i in range(len(ds)):\n",
    "        seeds.append({\n",
    "            'context': ds[i]['context'],\n",
    "            'instruction': ds[i]['question'],\n",
    "            'response': ds[i][model_name],\n",
    "            'stage': stage,\n",
    "            'age_group': ds[i]['age_group'],\n",
    "            'skill': ds[i]['skill'],\n",
    "            'subskill': ds[i]['subskill'],\n",
    "            'goal': ds[i]['goal'],\n",
    "            'indicator': ds[i]['indicator'],\n",
    "            'q_index': ds[i]['q_index']\n",
    "        })\n",
    "        \n",
    "    with open(f\"{root_dir}/seed_{model_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(seeds, f)\n",
    "\n",
    "def prepare_seed_cqa(stage, split_type, model_name):\n",
    "    ds = load_dataset(f\"Pavankalyan/stage{stage}_cqa_eval\", split=split_type)\n",
    "    root_dir = \"\"\n",
    "    seeds = []\n",
    "    for i in range(len(ds)):\n",
    "        seeds.append({\n",
    "            'context': ds[i]['context'],\n",
    "            'question': ds[i]['question'],\n",
    "            'answer': ds[i][model_name],\n",
    "            'stage': stage,\n",
    "            'age_group': ds[i]['age_group'],\n",
    "            'q_index': ds[i]['q_index']\n",
    "        })\n",
    "        \n",
    "    with open(f\"{root_dir}/seed_{model_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(seeds, f)\n",
    "        \n",
    "def prepare_seed_ir(stage, split_type, model_name):\n",
    "    ds = load_dataset(f\"Pavankalyan/stage{stage}_ir_eval\", split=split_type)\n",
    "    root_dir = \"\"\n",
    "    seeds = []\n",
    "    for i in range(len(ds)):\n",
    "        seeds.append({\n",
    "            'question': ds[i]['question'],\n",
    "            'answer': ds[i][model_name],\n",
    "            'stage': stage,\n",
    "            'age_group': ds[i]['age_group'],\n",
    "            'q_index': ds[i]['q_index']\n",
    "        })\n",
    "        \n",
    "    with open(f\"{root_dir}/seed_{model_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(seeds, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "res_path = f\"outputs_stage/instruct\"\n",
    "hf_df = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=os.path.join(res_path, \"*.parquet\"),\n",
    "        streaming=False\n",
    "    )\n",
    "\n",
    "def extract_details(row):\n",
    "    seed_data = row['user']\n",
    "    f1 = '''\\nIndex: '''\n",
    "    f2 = '''**Output Format:**'''\n",
    "    s = seed_data.find(f1)\n",
    "    e = seed_data.find(f2)\n",
    "    q_in = seed_data[s+len(f1):e].strip()\n",
    "    return {\"q_index\": q_in}\n",
    "\n",
    "def parse_json_string(text):\n",
    "    try:\n",
    "        cleaned = text.strip()\n",
    "        cleaned = re.sub(r'^```json\\s*', '', cleaned, flags=re.MULTILINE)\n",
    "        cleaned = re.sub(r'^```', '', cleaned, flags=re.MULTILINE)\n",
    "        cleaned = re.sub(r'```$', '', cleaned, flags=re.MULTILINE)\n",
    "\n",
    "        parsed = json.loads(cleaned)\n",
    "        return {\n",
    "            \"rating\": parsed.get(\"rating\"),\n",
    "            \"explanation\": parsed.get(\"explanation\")\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        def extract_int(key):\n",
    "            match = re.search(rf'\"{key}\"\\s*:\\s*(\\d+)', cleaned)\n",
    "            return int(match.group(1)) if match else None\n",
    "\n",
    "        rating = extract_int(\"rating\")\n",
    "        return {\n",
    "            \"rating\": rating,\n",
    "            \"explanation\": None\n",
    "        }\n",
    "\n",
    "def flatten_parsed_fields(example):\n",
    "    parsed = parse_json_string(example['answer'])\n",
    "    return {\n",
    "        \"rating\": parsed.get(\"rating\")\n",
    "    }\n",
    "\n",
    "hf_df = hf_df['train'].map(flatten_parsed_fields)\n",
    "hf_df = hf_df.remove_columns(['batch_uuid', 'embeddings', 'generated_tokens', 'messages', 'metrics', 'num_generated_tokens', 'num_input_tokens', 'params', 'prompt', 'prompt_token_ids', 'request_id', 'system', 'time_taken_llm'])\n",
    "hf_df = hf_df.map(extract_details)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
